{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11076276-5b4d-461c-ab62-8a7226236dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "os.environ[\"TF_FORCE_UNIFIED_MEMORY\"]=\"1\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"16.0\"\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_addons as tfa\n",
    "from einops.layers.tensorflow import Rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3e2d0c-73c7-432b-a66c-1ed14171467e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_dim, num_heads, mlp_ratio=4, drop_rate=0.0, **kwargs):\n",
    "        super(SwinTransformerBlock, self).__init__(**kwargs)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.drop_rate = drop_rate\n",
    "\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-5)\n",
    "        self.attention = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=hidden_dim)\n",
    "        self.drop1 = tf.keras.layers.Dropout(drop_rate)\n",
    "\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-5)\n",
    "        self.mlp = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(mlp_ratio * hidden_dim, activation=tf.keras.activations.gelu),\n",
    "            tf.keras.layers.Dense(hidden_dim),\n",
    "        ])\n",
    "        self.drop2 = tf.keras.layers.Dropout(drop_rate)\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        residual = x\n",
    "        x = self.norm1(x, training=training)\n",
    "        x = self.attention(x, x, x)\n",
    "        x = self.drop1(x, training=training)\n",
    "        x += residual\n",
    "\n",
    "        residual = x\n",
    "        x = self.norm2(x, training=training)\n",
    "        x = self.mlp(x)\n",
    "        x = self.drop2(x, training=training)\n",
    "        x += residual\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class SwinTransformer(tf.keras.Model):\n",
    "    def __init__(self, input_shape, window_size, patch_size, hidden_dim, num_heads, num_layers, channels, num_classes):\n",
    "        super(SwinTransformer, self).__init__()\n",
    "\n",
    "        self.window_size = window_size\n",
    "        self.patch_size = patch_size\n",
    "        self.channels = channels\n",
    "\n",
    "        self.patch_proj = tf.keras.Sequential([\n",
    "            Rearrange('b (h p1) (w p2) c -> b h w (p1 p2 c)', p1=patch_size, p2=patch_size),\n",
    "            tf.keras.layers.Dense(hidden_dim),\n",
    "        ])\n",
    "\n",
    "        self.blocks = [\n",
    "            SwinTransformerBlock(hidden_dim, num_heads, mlp_ratio=4, drop_rate=0.0)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "\n",
    "        self.final_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5)\n",
    "\n",
    "        self.proj = tf.keras.layers.Dense(channels)\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        x = self.patch_proj(x)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x, training=training)\n",
    "\n",
    "        x = self.final_norm(x, training=training)\n",
    "        x = self.proj(x)\n",
    "        x = tf.keras.activations.sigmoid(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc6bf64-751b-4927-b031-650f9b4049d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "BATCH_SIZE = 1\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 1e-4\n",
    "DIV2K_SCALE = 2\n",
    "\n",
    "# Load the DIV2K dataset\n",
    "def load_div2k(scale=2):\n",
    "    train, valid = tfds.load(\"div2k/bicubic_x2\", split=[\"train\", \"validation\"], as_supervised=True)\n",
    "\n",
    "    def preprocessing(lr, hr):\n",
    "        lr = tf.cast(lr, tf.float32) / 255.0\n",
    "        hr = tf.cast(hr, tf.float32) / 255.0\n",
    "        return lr, hr\n",
    "\n",
    "    train = train.map(preprocessing, num_parallel_calls=tf.data.AUTOTUNE).cache().shuffle(100).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "    valid = valid.map(preprocessing, num_parallel_calls=tf.data.AUTOTUNE).cache().batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return train, valid\n",
    "\n",
    "train_dataset, valid_dataset = load_div2k(DIV2K_SCALE)\n",
    "\n",
    "# Build the Swin Transformer model\n",
    "def build_swin_transformer_model(scale=2):\n",
    "    input_shape = (None, None, 3)\n",
    "\n",
    "    lr_input = tf.keras.layers.Input(shape=input_shape)\n",
    "    lr_upscaled = tf.keras.layers.UpSampling2D(size=(scale, scale), interpolation='bilinear')(lr_input)\n",
    "    \n",
    "    swin_transformer = SwinTransformer(\n",
    "        input_shape=input_shape,\n",
    "        window_size=8,\n",
    "        patch_size=4,\n",
    "        hidden_dim=128,\n",
    "        num_heads=4,\n",
    "        num_layers=2,\n",
    "        channels=3,\n",
    "        num_classes=0\n",
    "    )\n",
    "\n",
    "    hr_output = swin_transformer(lr_upscaled)\n",
    "\n",
    "    model = tf.keras.Model(inputs=lr_input, outputs=hr_output)\n",
    "    return model\n",
    "\n",
    "model = build_swin_transformer_model(DIV2K_SCALE)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss = tf.keras.losses.MeanAbsoluteError()\n",
    "optimizer = tfa.optimizers.AdamW(learning_rate=LEARNING_RATE, weight_decay=1e-4)\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer=optimizer, loss=loss)\n",
    "model.fit(train_dataset, epochs=EPOCHS, validation_data=valid_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
